{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the earthquake data from http://earthquake.usgs.gov API for this year:\n",
    "http://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&starttime=2016-01-01&endtime=2016-01-31\n",
    "\n",
    "Upload the file to your object store, named it as earthquake.csv for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val earthquakes = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"swift://notebooks.spark/earthquake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "earthquakes.registerTempTable(\"earthquake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- depth: double (nullable = true)\n",
      " |-- mag: double (nullable = true)\n",
      " |-- magType: string (nullable = true)\n",
      " |-- nst: integer (nullable = true)\n",
      " |-- gap: double (nullable = true)\n",
      " |-- dmin: double (nullable = true)\n",
      " |-- rms: double (nullable = true)\n",
      " |-- net: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- updated: string (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- horizontalError: double (nullable = true)\n",
      " |-- depthError: double (nullable = true)\n",
      " |-- magError: double (nullable = true)\n",
      " |-- magNst: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- locationSource: string (nullable = true)\n",
      " |-- magSource: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "earthquakes.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below query will fetch all the data about earthquakes that happened in California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val results = sqlContext.sql(\"select earthquake.* from earthquake where earthquake.place like '%California%'\")\n",
    "results.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below block is for using Machine learning Lib.\n",
    "Belowtwo import are the alogirithm that Spark has provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.clustering.KMeans\n",
    "import org.apache.spark.mllib.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val earthquakeFile = sc.textFile(\"swift://notebooks.spark/earthquake.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the CSV structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(time,latitude,longitude,depth,mag,magType,nst,gap,dmin,rms,net,id,updated,place,type,horizontalError,depthError,magError,magNst,status,locationSource,magSource)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthquakeFile.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val californiaData=earthquakeFile.filter(_.contains(\"California\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above statement filters the file data for California state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val caliDataOn30th  = californiaData.filter(_.contains(\"reviewed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above statement further filters the filedata for the earthquackes in California state with type 'reviewed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find maximum of depth values, Create a Vector of this data as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val caliFilteredDataVetcor = caliDataOn30th.map(line=>Vectors.dense(line.split(',').slice(3,4).map(_.toDouble)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train\n",
    "public static KMeansModel train(RDD<Vector> data,\n",
    "                int k,\n",
    "                int maxIterations)\n",
    "Trains a k-means model using specified parameters and the default values for unspecified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.257182741116755\n"
     ]
    }
   ],
   "source": [
    "val model=KMeans.train(caliFilteredDataVetcor,1,10)\n",
    "val clusterCenters=model.clusterCenters.map(_.toArray)\n",
    "clusterCenters.foreach(lines=>println(lines(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10 with Spark 1.6",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}